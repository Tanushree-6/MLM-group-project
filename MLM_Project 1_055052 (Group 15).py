# -*- coding: utf-8 -*-
"""Build Unsupervised Learning Models using Python- Rahul Bajaj (055036) Tanushree Nangia (055052) .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hD-YDTEPjYPqhyX_1q4_GqUY0asfVolw

# 1. **Project Information**

### **Project Title :** Build Unsupervised Learning Models using Python:Clustering {Hierarchical | K-Means}

### **Student Name(s) & Enrollment Number(s) :** Rahul Bajaj (055036) and Tanushree Nangia (055052)

### **Group Number -** 15

# 2. **Description of Data**

### **Data Source -** [Imports - Exports](https://www.kaggle.com/datasets/chakilamvishwas/imports-exports-15000?resource=download)
### **Data Size -** 2.56 MB
### **Data Type -**  Panel
### **Data Dimension -** [Number of Variables - 16 | Number of Observations - 5000
### **Data Variable Type -** [Numeric {Integer | Decimal} | Non-Numeric]
### **Data Variable Category-I -** [Index | Categorical {Nominal | Ordinal} | Non-Categorical]
### **Data Variable Category-II -** [Input Variables or Features | Outcome Variable(s) or Feature(s)]
### **About Dataset -** This dataset provides detailed information on international trade transactions, capturing both import and export activities. It includes comprehensive data on various aspects of trade, making it a valuable resource for business analysis, economic research, and financial modeling.

###Features:

#####**Transaction_ID:** Unique identifier for each trade transaction.
#####**Country:** Country of origin or destination for the trade.
#####**Product:** Product being traded.
#####**Import_Export:** Indicates whether the transaction is an import or export.
#####**Quantity:** Amount of the product traded.
#####**Value:** Monetary value of the product in USD.
#####**Date:** Date of the transaction.
#####**Category:** Category of the product (e.g., Electronics, Clothing, Machinery).
#####**Port:** Port of entry or departure.
#####**Customs_Code:** Customs or HS code for product classification.
#####**Weight:** Weight of the product in kilograms.
#####**Shipping_Method:** Method used for shipping (e.g., Air, Sea, Land).
#####**Supplier:** Name of the supplier or manufacturer.
#####**Customer:** Name of the customer or recipient.
#####**Invoice_Number:** Unique invoice number for the transaction.
#####**Payment_Terms:** Terms of payment (e.g., Net 30, Net 60, Cash on Delivery).
###**Usage:** This dataset can be used for various analyses, including:

#####**Trade Analysis:** Understanding trade patterns and trends.
#####**Economic Research:** Studying the impact of trade on different economies.
#####**Financial Modeling:** Evaluating financial performance and risks associated with trade.
#####**Supply Chain Management:** Analyzing logistics and shipping methods.

# **3. Project Objectives | Problem Statement**s
### **3.1. Unsupervised Machine Learning: Clustering**
Segmentation of Dataset using Unsupervised Machine Learning Clustering Algorithms
Identification of appropriate number of Segments or Clusters
Determination of Segment or Cluster Characteristics
### **3.2. Supervised Machine Learning: Classification & Regression**
Classification of Dataset into {Segments | Clusters | Classes} using Supervised Learning Classification Algorithms
Identification of {Important | Contributing | Significant} Variables or Features and their Thresholds for Classification
Determination of an appropriate Classification Model based on Performance Metrics

# **4. Observations | Findings**

## **4.1. Nature of Data**

### **●	Report on Missing Data.** [Link](https://colab.research.google.com/drive/1hD-YDTEPjYPqhyX_1q4_GqUY0asfVolw#scrollTo=r00pUgewe85w&line=3&uniqifier=1)

#### Variable-wise Missing Data Information (Percentage):
This report highlights the percentage of missing data for each variable in the dataset. All variables have 0% missing data, indicating a complete dataset.

| **Variable**              | **Missing Percentage (%)** |
|---------------------------|----------------------------|
| Transaction_ID            | 0.0                        |
| Country                   | 0.0                        |
| Product                   | 0.0                        |
| Import_Export             | 0.0                        |
| Quantity                  | 0.0                        |
| Value                     | 0.0                        |
| Date                      | 0.0                        |
| Category                  | 0.0                        |
| Port                      | 0.0                        |
| Customs_Code              | 0.0                        |
| Weight                    | 0.0                        |
| Shipping_Method           | 0.0                        |
| Supplier                  | 0.0                        |
| Customer                  | 0.0                        |
| Invoice_Number            | 0.0                        |
| Payment_Terms             | 0.0                        |
| Year                      | 0.0                        |
| Weight_Category           | 0.0                        |
| PCA_1                     | 0.0                        |
| PCA_2                     | 0.0                        |
| KMeans_Cluster            | 0.0                        |
| Hierarchical_Cluster      | 0.0                        |
| DBSCAN_Cluster            | 0.0                        |
| BIRCH_Cluster             | 0.0                        |

#### Record-wise Missing Data Information (Percentage):
All records have 0% missing data, ensuring the dataset is fully populated across all entries.

#### Processed Data Overview:
The dataset has 5,001 entries and 24 columns. Below is the summary of the processed data:

- **Index**: Ranges from 14442 to 10796.
- **Total Columns**: 24.
- **Data Types**:
  - category: 1
  - datetime64[ns]: 1
  - float64: 4
  - int32: 2
  - int64: 6
  - object: 10
- **Memory Usage**: Approximately 903.7 KB.

##### Dataset Columns:
- **Transaction_ID**: Unique identifier for transactions (object type).
- **Country**: Country of the transaction (object type).
- **Product**: Product involved in the transaction (object type).
- **Import_Export**: Indicates whether the transaction is an import or export (object type).
- **Quantity**: Number of items in the transaction (int64 type).
- **Value**: Monetary value of the transaction (float64 type).
- **Date**: Date of the transaction (datetime64[ns] type).
- **Category**: Product category (object type).
- **Port**: Port of entry or exit (object type).
- **Customs_Code**: Associated customs code (int64 type).
- **Weight**: Weight of the shipment (float64 type).
- **Shipping_Method**: Method of shipping (object type).
- **Supplier**: Supplier involved (object type).
- **Customer**: Customer involved (object type).
- **Invoice_Number**: Invoice number (int64 type).
- **Payment_Terms**: Payment terms for the transaction (object type).
- **Year**: Year extracted from the transaction date (int32 type).
- **Weight_Category**: Categorized weight (category type).
- **PCA_1**: Principal Component 1 (float64 type).
- **PCA_2**: Principal Component 2 (float64 type).
- **KMeans_Cluster**: Cluster label from KMeans (int32 type).
- **Hierarchical_Cluster**: Cluster label from hierarchical clustering (int64 type).
- **DBSCAN_Cluster**: Cluster label from DBSCAN (int64 type).
- **BIRCH_Cluster**: Cluster label from BIRCH (int64 type).

---

### **●	Report on Non-Numeric Categorical Data.** [Link](https://colab.research.google.com/drive/1hD-YDTEPjYPqhyX_1q4_GqUY0asfVolw#scrollTo=BR0L0oCvhQIL&line=1&uniqifier=1)

#### Categorical Data Overview:
The dataset contains several non-numeric categorical variables that describe various aspects of the transactions. Below is a detailed analysis:

| **Variable**        | **Description**                                             |
|---------------------|-------------------------------------------------------------|
| Transaction_ID      | Unique identifier for each transaction.                     |
| Country             | Country where the transaction occurred.                     |
| Product             | Type of product involved in the transaction.                |
| Import_Export       | Indicates whether the transaction is an import or export.  |
| Date                | The date of the transaction (datetime).                     |
| Category            | Category of the product (e.g., Furniture, Machinery, etc.). |
| Port                | Port of entry or exit for the transaction.                  |
| Shipping_Method     | Method of shipping (e.g., Land, Sea, Air).                 |
| Supplier            | Supplier associated with the transaction.                   |
| Customer            | Customer associated with the transaction.                   |
| Payment_Terms       | Terms of payment (e.g., Net 30, Prepaid, etc.).             |

#### Top 5 Rows of Categorical Data:
The first five rows of the categorical data provide insight into the dataset:

| **Transaction_ID**                                | **Country**     | **Product** | **Import_Export** | **Date**     | **Category** | **Port**           | **Shipping_Method** | **Supplier**              | **Customer**         | **Payment_Terms** |
|---------------------------------------------------|-----------------|-------------|-------------------|--------------|--------------|--------------------|---------------------|---------------------------|----------------------|-------------------|
| 3206e6b7-4544-4a7b-b9ca-fd0e94ff1daf             | Cote d'Ivoire   | physical    | Import            | 2022-06-14   | Furniture    | South Matthewville | Land                | Chavez-Ashley            | Jaime Klein          | Net 60            |
| 3ffac0d9-b35b-47d6-a045-c088f5624e91             | Aruba           | first       | Import            | 2022-07-22   | Furniture    | Port Michaelfort    | Sea                 | Powell, Smith and Larson | Raymond Odom         | Net 30            |
| 09aedbd0-6d31-4b4a-bf6b-8793b318ad4c             | Monaco          | morning     | Import            | 2022-02-26   | Machinery    | Kelseyborough      | Air                 | Graves, Hensley and Molina | Mr. Matthew Reyes MD | Net 60            |
| 691c9e37-45b0-46d9-aeae-7cf6269116b5             | Cyprus          | stand       | Export            | 2022-01-13   | Toys         | East Jessica       | Air                 | Nichols, Smith and Rivera | Donald Fletcher      | Prepaid           |
| 7bf04a96-64a0-4784-9b5b-6b0ad807fde9             | Australia       | hundred     | Export            | 2020-01-30   | Machinery    | Melanieview        | Sea                 | Jones, Stewart and Mccormick | Jacob Miller        | Cash on Delivery |

#### Encoded Categorical Data:
Categorical data was encoded into numeric values for analytical and modeling purposes. Below is an example of the encoded data for the first five rows:

| **Transaction_ID** | **Country** | **Product** | **Import_Export** | **Date** | **Category** | **Port** | **Shipping_Method** | **Supplier** | **Customer** | **Payment_Terms** |
|--------------------|-------------|-------------|-------------------|----------|--------------|----------|---------------------|--------------|--------------|-------------------|
| 927.0              | 52.0        | 608.0       | 1.0               | 750.0    | 2.0          | 3583.0   | 1.0                 | 687.0        | 1792.0       | 2.0               |
| 1216.0             | 11.0        | 310.0       | 1.0               | 1213.0   | 2.0          | 3021.0   | 2.0                 | 3236.0       | 3768.0       | 1.0               |
| 153.0              | 141.0       | 515.0       | 1.0               | 1413.0   | 3.0          | 1403.0   | 0.0                 | 1472.0       | 3478.0       | 2.0               |
| 2032.0             | 55.0        | 790.0       | 0.0               | 671.0    | 4.0          | 687.0    | 0.0                 | 2923.0       | 1327.0       | 3.0               |
| 2408.0             | 12.0        | 391.0       | 0.0               | 1632.0   | 3.0          | 1980.0   | 2.0                 | 2091.0       | 1781.0       | 0.0               |

---

###  **●	Report on Scales of & Outliers in Non-Categorical Numeric Data** [Link](https://colab.research.google.com/drive/1hD-YDTEPjYPqhyX_1q4_GqUY0asfVolw#scrollTo=_RBxeo10lyDg&line=1&uniqifier=1)

#### Overview of Non-Categorical Numeric Data:
The non-categorical numeric variables analyzed are:
- **Quantity**: Represents the number of items.
- **Value**: Represents the monetary value of the transactions.
- **Weight**: Represents the weight of the goods.

#### Outlier Analysis:

| **Variable** | **Scale** | **Num_Outliers** | **Outlier_Values** | **Lower Bound** | **Upper Bound** |
|--------------|-----------|------------------|--------------------|-----------------|-----------------|
| Quantity     | Ratio     | 0                | [ ]                | -4867.00        | 14829.00        |
| Value        | Ratio     | 0                | [ ]                | -4982.55        | 14914.09        |
| Weight       | Ratio     | 0                | [ ]                | -2580.66        | 7609.90         |

##### Observation:
No outliers were detected in any of the variables based on the lower and upper bounds calculated using the Interquartile Range (IQR) method. The data appears to be well-contained within reasonable bounds.

#### Standardized Data:
Standardization transforms the data to have a mean of 0 and a standard deviation of 1. This is useful for comparing variables on different scales.

| **Index** | **Quantity** | **Value** | **Weight** |
|-----------|--------------|-----------|------------|
| 14442     | -0.377337    | -1.181221 | -1.288778  |
| 9008      | -1.607304    | -0.761307 | -1.107889  |
| 148       | -0.798124    | 1.506552  | -0.324343  |
| 5543      | -0.858386    | -1.440994 | 0.180831   |
| 6114      | -0.358875    | -1.379268 | -0.293250  |

##### Observation:
Negative values indicate data points below the mean, while positive values are above the mean. Standardized data can be used for machine learning models sensitive to feature scaling.

#### Normalized Data:
Normalization rescales the data to a range of 0 to 1. This is useful for algorithms that require bounded inputs.

| **Index** | **Quantity** | **Value** | **Weight** |
|-----------|--------------|-----------|------------|
| 14442     | 0.390935     | 0.151709  | 0.127401   |
| 9008      | 0.037623     | 0.272986  | 0.179992   |
| 148       | 0.270062     | 0.927976  | 0.407798   |
| 5543      | 0.252752     | 0.076683  | 0.554670   |
| 6114      | 0.396238     | 0.094510  | 0.416838   |

#### Observation:
Normalized data is particularly useful for visualizations and models that rely on distance metrics (e.g., K-means).

#### Log-Transformed Data:
Log transformation reduces skewness and compresses the range of data, making it more suitable for analysis.

| **Index** | **Quantity** | **Value** | **Weight** |
|-----------|--------------|-----------|------------|
| 14442     | 8.271037     | 7.380729  | 6.459028   |
| 9008      | 5.934894     | 7.939009  | 6.803883   |
| 148       | 7.901377     | 9.136106  | 7.620769   |
| 5543      | 7.835184     | 6.759847  | 7.928168   |
| 6114      | 8.284504     | 6.945898  | 7.642678   |

#### Observation:
Log transformation is effective in stabilizing variance and handling large ranges in data. Useful for regression models and other algorithms sensitive to data skewness.

---

### Conclusions:
- **Outliers**: None of the variables have significant outliers, indicating well-behaved data.
- **Scaling**: Standardization, normalization, and log transformations provide different perspectives and are useful depending on the analysis or modeling requirements.

#### Recommendation:
- Use **standardized data** for machine learning models like SVM or Logistic Regression.
- **Normalize** data for clustering or visualization tasks.
- Apply **log transformation** for regression models or when handling skewed data.

## **4.2. Unsupervised Machine Learning: Clustering**

### ●	For each Clustering Model, Determination of appropriate number of Clusters, based on Performance Metrics.

#### 1. Hierarchical Clustering [Link](https://colab.research.google.com/drive/1hD-YDTEPjYPqhyX_1q4_GqUY0asfVolw#scrollTo=gTNGfrwCVhH5&line=1&uniqifier=1)

#####  Approach:
- Hierarchical clustering creates a dendrogram to visualize how clusters are formed.
- Performance metrics like the Silhouette Coefficient and Davies-Bouldin Index are useful for assessing cluster quality.

#####  Performance Insights:
- **Silhouette Coefficient (0.31)**: Indicates that clusters are poorly defined, with significant overlap or ambiguity in cluster assignments.
- **Davies-Bouldin Index (1.04)**: Suggests clusters are not well-separated, indicating room for improvement in inter-cluster distances.
[Link](https://colab.research.google.com/drive/1hD-YDTEPjYPqhyX_1q4_GqUY0asfVolw#scrollTo=NzdvS1giWhVY&line=15&uniqifier=1)
##### Recommendation:
- Analyze the dendrogram to test different numbers of clusters (e.g., 2–5) and reassess the performance metrics.
- Experiment with different linkage methods and distance metrics.

#### 2. K-Means Clustering [Link](https://colab.research.google.com/drive/1hD-YDTEPjYPqhyX_1q4_GqUY0asfVolw#scrollTo=3CqA_NlVQkPl&line=1&uniqifier=1)

##### Approach:
- K-Means clustering assigns data points to the nearest centroid.
- The Elbow Method, Silhouette Coefficient, and Davies-Bouldin Index are used to determine the optimal number of clusters.

##### Performance Insights:
- **Silhouette Coefficient (0.31)**: Indicates clusters are poorly defined, suggesting overlap between clusters or inconsistent data distribution.
- **Davies-Bouldin Index (1.04)**: Indicates clusters are too similar or compact, requiring adjustments to cluster assignments.

##### Recommendation:
- Reassess the number of clusters using the elbow method for `k=2` to `k=10`.
- Use silhouette scores and Davies-Bouldin Index to validate `n_clusters`.
- Consider scaling the data differently to improve clustering performance.

#### 3. DBSCAN [Link](https://colab.research.google.com/drive/1hD-YDTEPjYPqhyX_1q4_GqUY0asfVolw#scrollTo=9DKzALHbV7Tu&line=1&uniqifier=1)

##### Approach:
- DBSCAN identifies clusters based on density, eliminating the need to predefine the number of clusters.

##### Performance Insights:
- Current Silhouette Coefficient and Davies-Bouldin Index suggest that density-based clusters may not be well-defined.
- DBSCAN's ability to handle noise and outliers could improve overall clustering performance if parameters are optimized.

##### Recommendation:
- Generate a k-distance plot to determine the optimal `eps` value.
- Start with `min_samples = 4` or 5 and adjust to find a balance between meaningful clusters and noise points.
- Reassess performance metrics after parameter tuning.

#### 4. BIRCH [Link](https://colab.research.google.com/drive/1hD-YDTEPjYPqhyX_1q4_GqUY0asfVolw#scrollTo=7KKghSwKWCiZ&line=1&uniqifier=1)

##### Approach:
- BIRCH uses hierarchical clustering principles to form clusters iteratively.
- The threshold parameter controls sub-cluster granularity, which affects the final clustering.

##### Performance Insights:
- Silhouette Coefficient and Davies-Bouldin Index suggest suboptimal cluster definitions.
- Adjusting the threshold parameter could lead to better-defined clusters.

##### Recommendation:
- Experiment with threshold values to optimize cluster formation.
- Use silhouette scores and Davies-Bouldin Index to validate clustering quality for different thresholds.

#### 5. PCA with K-Means Clustering [Link](https://colab.research.google.com/drive/1hD-YDTEPjYPqhyX_1q4_GqUY0asfVolw#scrollTo=FXlgS94BVW0I&line=1&uniqifier=1)

##### Approach:
- PCA reduces dimensionality, enabling easier visualization and clustering in a lower-dimensional space.
- Performance metrics assess how well the reduced dimensions capture the data's structure.

##### Performance Insights:
- PCA might distort distances, affecting the clustering performance as seen in the low Silhouette Coefficient and high Davies-Bouldin Index.
- Retaining more variance in PCA components could improve clustering quality.

##### Recommendation:
- Ensure PCA components explain at least 80–90% of the variance.
- Reassess clustering performance metrics after adjusting the number of PCA components.

### ●	Discovery of Niche Clusters (if any).
#### Niche Clusters Analysis

##### 1. K-Means Clustering
- **Cluster Distribution**: K-Means divides the data into 3 clusters. Based on the Silhouette Coefficient (0.31), these clusters are not well-defined, suggesting potential overlaps. However, some clusters may still exhibit niche characteristics, especially when analyzing the distribution of features like 'Quantity' and 'Value.'
  
- **Potential Niche Clusters**:
  - If any cluster shows extreme values in 'Quantity' and 'Value' (e.g., high-value low-quantity or low-value high-quantity), it could represent a niche market.
  - For example, a cluster where 'Quantity' is low but 'Value' is high could indicate high-end or luxury products, which might be a niche.

##### 2. DBSCAN (Density-Based Spatial Clustering of Applications with Noise)
- **Noise Points**: DBSCAN identifies noise points (labeled as '-1') and clusters based on density. These noise points might indicate outliers or niche observations that do not fit well into the general clustering structure.
  
- **Potential Niche Clusters**:
  - Points that are not part of any cluster (noise) could represent rare events or unique combinations of features that are outside the typical patterns observed in the data.
  - If a cluster is found to contain a very high density of points in a particular range of features (e.g., high 'Weight' but low 'Value'), it may represent a niche segment of the data.

##### 3. Hierarchical Clustering
- **Dendrogram Analysis**: Hierarchical clustering does not require predefining the number of clusters. By examining the dendrogram, we can identify where smaller, more specific clusters form. These could represent niche groups within the data.
  
- **Potential Niche Clusters**:
  - A subset of data points that forms a small, distinct branch in the dendrogram may indicate a niche cluster. For example, if a group of data points consistently shows unique combinations of 'Country' and 'Product,' it could represent a specific market or product category.

##### 4. BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies)
- **Hierarchical Clustering with BIRCH**: BIRCH forms clusters based on hierarchical principles, and adjusting the threshold parameter can help identify more granular clusters.
  
- **Potential Niche Clusters**:
  - Similar to hierarchical clustering, BIRCH can reveal sub-clusters within the data that might not be obvious at a higher level of clustering. Niche clusters may emerge when certain thresholds are fine-tuned, particularly in cases where there is a significant variance in features like 'Quantity' or 'Value.'


### ●	For each Clustering Model, Recognition of Distinguishing Features of each Cluster.

#### Cluster Analysis Summary

#### 1. K-Means Clustering
- **Cluster Characteristics**:
  - **Cluster 1**: High 'Quantity', Low 'Value'
    - **Distinguishing Features**:
      - Higher 'Quantity' of items sold or processed.
      - Lower 'Value' indicating bulk or low-cost items.
      - Likely associated with mass-market products or low-margin goods.
  - **Cluster 2**: Low 'Quantity', High 'Value'
    - **Distinguishing Features**:
      - Lower 'Quantity' of items sold, but each item is more expensive or valuable.
      - Likely to represent high-end products, luxury items, or specialized goods.
      - Higher profit margins.
  - **Cluster 3**: Moderate 'Quantity' and 'Value'
    - **Distinguishing Features**:
      - A balanced mix of both 'Quantity' and 'Value'.
      - Likely mid-range products with average pricing and volume.
      - Representative of mainstream products in the market.

- **General Distinguishing Features for K-Means**:
  - K-Means clustering works well when clusters are roughly spherical and of similar size. The clusters are formed based on proximity to centroids, with distinguishing features primarily based on the central tendency of 'Quantity' and 'Value'.

#### 2. DBSCAN (Density-Based Spatial Clustering of Applications with Noise)
- **Cluster Characteristics**:
  - **Cluster 1**: Dense regions with high 'Quantity' and low 'Value'
    - **Distinguishing Features**:
      - High-volume, low-value items.
      - Likely bulk products or low-cost transactions.
  - **Cluster 2**: Dense regions with low 'Quantity' and high 'Value'
    - **Distinguishing Features**:
      - Fewer data points but higher value per item.
      - Likely specialized or high-end products, representing niche markets or luxury goods with higher profit margins.
  - **Noise Points (Label -1)**:
    - **Distinguishing Features**:
      - Points that do not fit well into any cluster.
      - Could represent rare transactions, anomalies, or outliers.
      - Indicative of unusual transactions or events.

- **General Distinguishing Features for DBSCAN**:
  - DBSCAN identifies clusters based on density, allowing it to find arbitrarily shaped clusters and handle noise well. The distinguishing feature for DBSCAN clusters is the density of data points in specific regions of the feature space (e.g., 'Quantity' and 'Value').

#### 3. Hierarchical Clustering
- **Cluster Characteristics**:
  - **Cluster 1**: Low 'Quantity', High 'Value'
    - **Distinguishing Features**:
      - Items sold in low quantities but at a high price.
      - Likely luxury goods or highly specialized products.
  - **Cluster 2**: High 'Quantity', Low 'Value'
    - **Distinguishing Features**:
      - High-volume, low-value items such as bulk goods or low-cost products.
      - Likely mass-market items or commodities.
  - **Cluster 3**: Moderate 'Quantity' and 'Value'
    - **Distinguishing Features**:
      - A balanced combination of both 'Quantity' and 'Value'.
      - Likely mid-range products or products appealing to a broad consumer base.

- **General Distinguishing Features for Hierarchical Clustering**:
  - Hierarchical clustering forms a tree-like structure, where clusters with similar characteristics (e.g., 'Quantity' and 'Value') tend to merge first, forming distinct groups at higher levels of the tree.

#### 4. BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies)
- **Cluster Characteristics**:
  - **Cluster 1**: High 'Quantity', Low 'Value'
    - **Distinguishing Features**:
      - Bulk products or low-cost items with high volume but low individual value.
      - Likely representative of mass-market or low-margin products.
  - **Cluster 2**: Low 'Quantity', High 'Value'
    - **Distinguishing Features**:
      - Products with low volume but high value, indicating luxury or specialized products.
      - Could represent niche or premium goods.
  - **Cluster 3**: Moderate 'Quantity' and 'Value'
    - **Distinguishing Features**:
      - A balanced combination of 'Quantity' and 'Value'.
      - Likely mainstream products or those with an average price point.

- **General Distinguishing Features for BIRCH**:
  - BIRCH uses hierarchical principles for clustering, making it efficient for large datasets. The distinguishing features of each cluster depend on the characteristics of subclusters and how they are merged. BIRCH is good at identifying compact clusters, and its distinguishing features depend on how well-separated the clusters are in terms of 'Quantity' and 'Value'.

#### Summary of Distinguishing Features

| **Clustering Model** | **Cluster Characteristics** | **Distinguishing Features** |
|----------------------|-----------------------------|-----------------------------|
| **K-Means** | 3 clusters: High-Quantity/Low-Value, Low-Quantity/High-Value, Moderate Quantity/Value | Based on centroids, clusters with extreme or balanced values of 'Quantity' and 'Value'. |
| **DBSCAN** | Dense clusters, Noise points | Identifies clusters based on density, with noise points as outliers. Clusters with high or low density in 'Quantity' and 'Value'. |
| **Hierarchical** | 3 clusters: Low-Quantity/High-Value, High-Quantity/Low-Value, Moderate Quantity/Value | Based on hierarchical merging, distinguishing clusters with different levels of 'Quantity' and 'Value'. |
| **BIRCH** | 3 clusters: High-Quantity/Low-Value, Low-Quantity/High-Value, Moderate Quantity/Value | Hierarchical principles, compact clusters based on 'Quantity' and 'Value'. |

# **5. Managerial Insights | Recommendations**
### **5.1. Preprocessing of Data**

### ●	Recommended Treatment of Missing Data.
Based on the dataset and its features, the following strategies are recommended for handling missing data:

- **Numerical Features**:
  - **Imputation Methods**:
    - **Mean/Median Imputation**: For numerical columns like 'Quantity', 'Value', and 'Weight', the median is preferred to avoid skewing the data due to outliers.
  - **Mode Imputation**: For features with categorical characteristics (e.g., 'Shipping_Method', 'Payment_Terms'), the mode (most frequent value) should be used for imputation.

- **Categorical Features**:
  - **Imputation Methods**:
    - **Mode Imputation**: Missing entries in categorical features such as 'Product', 'Country', and 'Port' can be imputed with the mode.
    - If a high proportion of data is missing, excluding the feature or treating missing values as a separate category may be more effective.

### ●	Recommended Numerical Encoding of Non-Numeric Categorical Data.
For non-numeric categorical data, the following encoding techniques are recommended:

- **One-Hot Encoding**:
  - For features without an ordinal relationship (e.g., 'Country', 'Product', 'Category', 'Port', 'Supplier'), One-Hot Encoding creates binary columns for each category.
  
- **Label Encoding**:
  - For ordinal features (e.g., 'Payment_Terms' with inherent order such as 'Net 30', 'Net 60'), Label Encoding assigns an integer to each category based on its order.

- **Frequency Encoding**:
  - For features with many unique categories (e.g., 'Invoice_Number'), Frequency Encoding replaces each category with its frequency of occurrence.

### ●	Recommended Treatment of Non-Categorical Numeric Data having Non-Uniform Scales & Outliers.

To handle non-categorical numeric data with non-uniform scales and outliers, the following preprocessing steps are recommended:

- **Scaling of Data**:
  - **Standardization (Z-Score Normalization)**: For features like 'Quantity', 'Value', and 'Weight', standardization ensures each feature has a mean of 0 and a standard deviation of 1.
  - **Normalization (Min-Max Scaling)**: Apply normalization if required, especially for models sensitive to the magnitude of features, like neural networks.

- **Handling Outliers**:
  - **Winsorization**: Apply Winsorization to replace extreme values with the nearest acceptable value, reducing the impact of outliers.
  - **Log Transformation**: For features like 'Quantity' and 'Value', which may exhibit a right-skewed distribution, a log transformation can reduce the impact of large values and normalize the distribution.
  - **IQR Method**: Use the Interquartile Range (IQR) method to detect and handle outliers by identifying values outside the range of 1.5 times the IQR above the third quartile or below the first quartile.

### ●	Recommended Split [70:30 | 75:25 | 80:20] of Sample Data into Training & Testing Sets based on Method of Sampling (without replacement): Random (Clustering) | Stratified (Classification).

- **For Clustering Models (K-Means, DBSCAN, Hierarchical, BIRCH)**:
  - **Random Split**: Since clustering is unsupervised, splitting the data into training and testing sets is not strictly necessary. However, a **70:30 split** is recommended for validation purposes.
  
- **For Classification Models (if applicable in future analysis)**:
  - **Stratified Split**: If classification is used in the future, a **75:25 or 80:20 split** should be used to ensure each class is proportionally represented in both training and testing sets.

---
### **5.2. Unsupervised Machine Learning: Clustering**

### ●	Inference regarding the Purpose of Segmentation.
The purpose of segmentation in clustering is to divide the data into distinct groups or clusters based on similarities in their features. This enables businesses to:

- **Identify Patterns**: Discover hidden patterns, trends, and relationships in the data that can inform business strategies.
- **Targeted Marketing & Personalization**: Segment customers to tailor marketing efforts, offers, and product recommendations to each group.
- **Improved Decision-Making**: Make informed decisions by categorizing data into actionable groups for better resource allocation and strategy formulation.
- **Operational Efficiency**: Optimize processes like inventory management, logistics, and supply chain operations by aligning them with the needs of each cluster.

---

### ●	For each Clustering Model, Establishment of Homogeneous as well as Distinguishing Characteristics of each Cluster. Recommendation of Suggestive Name of each Cluster.

### ●	For each Clustering Model, Insight on Potential Use of each Cluster based on their Characteristics.

### 1. K-Means Clustering:
- **Homogeneous Characteristics**:
  - **Cluster 1**: Customers with low quantity and low value transactions, typically engaged in low-cost, high-frequency purchases.
  - **Cluster 2**: Customers with high quantity but moderate value transactions, often representing bulk buyers or wholesale transactions.
  - **Cluster 3**: Customers with high-value, low-quantity transactions, likely representing high-end or luxury product buyers.

- **Distinguishing Characteristics**:
  - **Cluster 1**: Small, frequent purchases; likely price-sensitive, looking for bulk deals or discounts.
  - **Cluster 2**: Bulk buyers or wholesalers; large quantity but moderate value transactions.
  - **Cluster 3**: Premium customers; high-value, low-quantity transactions, likely purchasing luxury goods.

- **Suggested Cluster Names**:
  - **Cluster 1**: "Frequent Low-Value Buyers"
  - **Cluster 2**: "Bulk Purchasers"
  - **Cluster 3**: "Premium Shoppers"

- **Potential Use**:
  - **Cluster 1**: Target with promotions, loyalty programs, or bulk discounts.
  - **Cluster 2**: Focus on wholesale pricing, B2B sales, and bulk order incentives.
  - **Cluster 3**: Offer high-end products, exclusive deals, or VIP customer services.

---

### 2. DBSCAN (Density-Based Clustering):
- **Homogeneous Characteristics**:
  - **Cluster 0**: Dense clusters of customers with similar purchase behaviors (moderate to high quantity and value).
  - **Noise Points (-1)**: Customers with unique or irregular purchasing patterns that do not fit into any cluster.

- **Distinguishing Characteristics**:
  - **Cluster 0**: Dense group of customers with regular and significant transactions, likely key account holders or frequent buyers.
  - **Noise Points**: Irregular transactions, possibly one-off purchases or errors.

- **Suggested Cluster Names**:
  - **Cluster 0**: "Core Customers"
  - **Noise Points**: "Outliers"

- **Potential Use**:
  - **Cluster 0**: Focus on retention, loyalty programs, and cross-selling/up-selling.
  - **Noise Points**: Investigate the reasons for irregular transactions and target with specific marketing campaigns.

---

### 3. Hierarchical Clustering:
- **Homogeneous Characteristics**:
  - **Cluster 1**: Customers with similar purchasing patterns, with moderate to high quantity and value.
  - **Cluster 2**: Customers with low quantity but high-value transactions, likely representing niche markets or high-end buyers.
  - **Cluster 3**: Customers with low quantity and low value, indicating low-frequency or price-sensitive buyers.

- **Distinguishing Characteristics**:
  - **Cluster 1**: Diverse set of customers with regular purchases at moderate to high values.
  - **Cluster 2**: High-value, low-quantity transactions, suggesting premium customers who prioritize quality.
  - **Cluster 3**: Low-value, low-quantity transactions, likely price-sensitive or irregular buyers.

- **Suggested Cluster Names**:
  - **Cluster 1**: "Regular Consumers"
  - **Cluster 2**: "Premium Shoppers"
  - **Cluster 3**: "Price-Sensitive Buyers"

- **Potential Use**:
  - **Cluster 1**: General promotions, loyalty programs, or seasonal offers.
  - **Cluster 2**: Exclusive deals, luxury products, or high-end customer services.
  - **Cluster 3**: Discounts, budget-friendly products, or clearance sales.

---

### 4. BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies):
- **Homogeneous Characteristics**:
  - **Cluster 1**: Customers with moderate quantity and value transactions, indicating steady purchasing behavior.
  - **Cluster 2**: Customers with high-value, low-quantity transactions, likely representing exclusive or niche customers.
  - **Cluster 3**: Customers with low-value and low-quantity transactions, possibly occasional buyers or bargain hunters.

- **Distinguishing Characteristics**:
  - **Cluster 1**: Balanced group of customers who purchase regularly but at moderate values.
  - **Cluster 2**: High-value, low-quantity transactions indicating a more exclusive customer base.
  - **Cluster 3**: Low-value, low-quantity transactions suggesting price-sensitive or infrequent buyers.

- **Suggested Cluster Names**:
  - **Cluster 1**: "Steady Buyers"
  - **Cluster 2**: "Exclusive Shoppers"
  - **Cluster 3**: "Occasional Bargain Hunters"

- **Potential Use**:
  - **Cluster 1**: Encourage engagement through loyalty programs, regular offers, and product bundles.
  - **Cluster 2**: Target high-end or exclusive products, personalized offers, and VIP customer services.
  - **Cluster 3**: Promote seasonal sales, limited-time offers, or discount-driven marketing.

# **6. Analysis of Data**
"""

import pandas as pd
#Import relevant libraries
import pandas as pd
import statsmodels.formula.api as sm
import statsmodels.stats.descriptivestats as dstats
import numpy as np  # For Data Manipulation
from sklearn.preprocessing import LabelEncoder, OrdinalEncoder  # For Encoding Categorical Data [Nominal | Ordinal]
from sklearn.preprocessing import OneHotEncoder  # For Creating Dummy Variables of Categorical Data [Nominal]
from sklearn.impute import SimpleImputer, KNNImputer  # For Imputation of Missing Data
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler  # For Rescaling Data
from sklearn.model_selection import train_test_split  # For Splitting Data into Training & Testing Sets
import matplotlib.pyplot as plt
import seaborn as sns  # For Data Visualization
import scipy.cluster.hierarchy as sch  # For Hierarchical Clustering
from sklearn.cluster import AgglomerativeClustering as agclus, KMeans as kmclus  # For Agglomerative & K-Means Clustering
from sklearn.metrics import silhouette_score as sscore, davies_bouldin_score as dbscore  # For Clustering Model Evaluation
import statsmodels.api as sm
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report # For Decision Tree Model Evaluation
from sklearn.tree import DecisionTreeClassifier, export_text, plot_tree  # For Decision Tree Model
import scipy.stats as stats
import statsmodels.stats.multicomp as multi
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier

from google.colab import drive
drive.mount('/content/drive')

rb36tn52file_path = '/content/drive/MyDrive/MLM/Imports_Exports_Dataset.csv'

# Read the dataset
rb36tn52_df = pd.read_csv(rb36tn52file_path)

# Display the first few rows of the DataFrame
rb36tn52_df.head()

# Displays a concise summary of the DataFrame rb36_df, including column names, non-null counts, and data types
rb36tn52_df.info()

#Creating unique sample of 5001
rb36tn52_dfs = rb36tn52_df.sample(5001, random_state = 55036)
rb36tn52_dfs

"""## **Data Preprocessing**
### Treatment of Missing Data
#### ●	Variables  & Records: Data Missing {>= 50% → Removal | < 50% → Imputation (Mean | Median | Mode)}
"""

# --- Missing Data Treatment ---

# Variable-wise missing data information (in percentage)
rb36tn52_variable_missing_percent = (rb36tn52_dfs.isnull().sum() / len(rb36tn52_dfs)) * 100
print("Variable-wise Missing Data Information (Percentage):")
print(rb36tn52_variable_missing_percent)

# Remove variables with missing data >= 50%
rb36tn52_dfs = rb36tn52_dfs.loc[:, rb36tn52_variable_missing_percent < 50]

# Record-wise missing data information (in percentage)
rb36tn52_record_missing_percent = (rb36tn52_dfs.isnull().sum(axis=1) / rb36tn52_dfs.shape[1]) * 100
print("\nRecord-wise Missing Data Information (Percentage):")
print(rb36tn52_record_missing_percent)

# Remove records with missing data >= 50%
rb36tn52_dfs = rb36tn52_dfs.loc[rb36tn52_record_missing_percent < 50]

# Impute missing data for remaining variables
for column in rb36tn52_dfs.columns:
    if rb36tn52_dfs[column].dtype in ['float64', 'int64']:  # Numerical columns
        rb36tn52_dfs[column] = rb36tn52_dfs[column].fillna(rb36tn52_dfs[column].mean())  # Replace with mean
    elif rb36tn52_dfs[column].dtype == 'object':  # Categorical columns
        rb36tn52_dfs[column] = rb36tn52_dfs[column].fillna(rb36tn52_dfs[column].mode()[0])  # Replace with mode

# --- Display Processed Data ---
print("\nProcessed Data:")
rb36tn52_dfs.info()

"""### **Numerical Encoding of Non-Numeric Categorical Data**"""

# Define lists of categorical and numerical columns
rb36tn52_categorical_cols = ['Transaction_ID', 'Country', 'Product', 'Import_Export', 'Date',
                             'Category', 'Port', 'Customs_Code', 'Shipping_Method', 'Supplier',
                             'Customer', 'Invoice_Number', 'Payment_Terms']
rb36tn52_numerical_cols = ['Quantity', 'Value', 'Weight']

# Subset the data with rb36tn52_ prefix for new DataFrames
rb36tn52_numerical_data = rb36tn52_dfs[rb36tn52_numerical_cols]
rb36tn52_categorical_data = rb36tn52_dfs[rb36tn52_categorical_cols]

# Display Subsetted Data
print("\nNumerical Data:")
print(rb36tn52_numerical_data.head())

print("\nCategorical Data:")
print(rb36tn52_categorical_data.head())

# Initialize OrdinalEncoder
rb36tn52_encoder = OrdinalEncoder()

# Fit and transform the categorical data
rb36tn52_encoded_data = rb36tn52_encoder.fit_transform(rb36tn52_categorical_data)

# Create a new DataFrame with encoded values and original column names
rb36tn52_encoded_df = pd.DataFrame(rb36tn52_encoded_data, columns=rb36tn52_categorical_data.columns,
                                   index=rb36tn52_categorical_data.index)  # Maintain original index

# Display the encoded data
print("\nEncoded Categorical Data:")
print(rb36tn52_encoded_df.head())

"""### **Re-Scaling & Transformation of Numeric Data (Treatment of Data having Outliers)**
### Standardization | Normalization (Min-Max Scaling) | Log Transformation

"""

def outlier_report(data):
    """Generates an outlier report for numerical features in a DataFrame.

    Args:
        data: pandas DataFrame containing numerical features.

    Returns:
        pandas DataFrame with outlier information for each feature.
    """
    report = []
    for col in data.columns:
        # Scale (assuming continuous data)
        scale = "Ratio" if data[col].min() >= 0 else "Interval"

        # Outlier detection using IQR
        Q1 = data[col].quantile(0.25)
        Q3 = data[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        outliers = data[(data[col] < lower_bound) | (data[col] > upper_bound)][col]

        report.append({
            "Variable": col,
            "Scale": scale,
            "Num_Outliers": len(outliers),
            "Outlier_Values": outliers.tolist(),
            "Lower_Bound": lower_bound,
            "Upper_Bound": upper_bound
        })

    return pd.DataFrame(report)

# Generate and print the report for numerical data
rb36tn52_outlier_report = outlier_report(rb36tn52_numerical_data)
print("Non-Categorical Data Outlier Report:")
print(rb36tn52_outlier_report)

def rescale_transform_data(data, method="standardization"):
    """Rescales and transforms numeric data.

    Args:
        data: pandas DataFrame containing numerical features.
        method: str, the transformation method ('standardization', 'normalization', 'log').

    Returns:
        pandas DataFrame with transformed data.
    """
    transformed_data = data.copy()

    if method == "standardization":
        # Standardization (Z-Score Scaling)
        scaler = StandardScaler()
        transformed_data = pd.DataFrame(
            scaler.fit_transform(data),
            columns=data.columns,
            index=data.index
        )
    elif method == "normalization":
        # Min-Max Scaling
        scaler = MinMaxScaler()
        transformed_data = pd.DataFrame(
            scaler.fit_transform(data),
            columns=data.columns,
            index=data.index
        )
    elif method == "log":
        # Log Transformation (Handle zeros or negative values separately)
        for col in data.columns:
            transformed_data[col] = np.log1p(data[col])  # log1p handles log(1 + x) to avoid log(0)
    else:
        raise ValueError("Invalid method. Choose 'standardization', 'normalization', or 'log'.")

    return transformed_data

# 1. Standardization
rb36tn52_standardized_data = rescale_transform_data(rb36tn52_numerical_data, method="standardization")
print("\nStandardized Data:")
print(rb36tn52_standardized_data.head())

# 2. Normalization
rb36tn52_normalized_data = rescale_transform_data(rb36tn52_numerical_data, method="normalization")
print("\nNormalized Data:")
print(rb36tn52_normalized_data.head())

# 3. Log Transformation
rb36tn52_log_transformed_data = rescale_transform_data(rb36tn52_numerical_data, method="log")
print("\nLog Transformed Data:")
print(rb36tn52_log_transformed_data.head())

"""## **Descriptive Statistics**

### **Categorical Variable [Nominal | Ordinal]**
#### {Count | Frequency | Proportion | Minimum | Maximum | Mode | Rank | Correlation (Spearman | Kendall)}
"""

def categorical_analysis(data, categorical_cols, numerical_cols=None):
    """Analyzes categorical variables for various metrics.

    Args:
        data: pandas DataFrame containing the dataset.
        categorical_cols: List of categorical columns to analyze.
        numerical_cols: List of numerical columns for correlation analysis (optional).

    Returns:
        Dictionary containing metrics for each categorical column.
    """
    analysis_results = {}

    for col in categorical_cols:
        col_data = data[col]
        result = {}

        # Count and Frequency
        value_counts = col_data.value_counts()
        result['Count'] = value_counts.to_dict()
        result['Frequency'] = value_counts / len(col_data)
        result['Proportion'] = (value_counts / len(col_data)).to_dict()

        # Minimum, Maximum, and Mode
        result['Minimum'] = col_data.min()
        result['Maximum'] = col_data.max()
        result['Mode'] = col_data.mode().iloc[0] if not col_data.mode().empty else None

        # Rank (based on frequency)
        result['Rank'] = value_counts.rank(ascending=False).to_dict()

        # Correlation (Spearman and Kendall) with numerical columns
        if numerical_cols:
            spearman_corr = {}
            kendall_corr = {}
            for num_col in numerical_cols:
                spearman_corr[num_col] = data[col].astype('category').cat.codes.corr(
                    data[num_col], method='spearman')
                kendall_corr[num_col] = data[col].astype('category').cat.codes.corr(
                    data[num_col], method='kendall')
            result['Spearman Correlation'] = spearman_corr
            result['Kendall Correlation'] = kendall_corr

        analysis_results[col] = result

    return analysis_results

# To display the metrics for each categorical variable in your dataset
rb36tn52_categorical_metrics = categorical_analysis(rb36tn52_dfs, rb36tn52_categorical_cols, rb36tn52_numerical_cols)

# Displaying the results for each categorical variable
for col, metrics in rb36tn52_categorical_metrics.items():
    print(f"\nMetrics for {col}:")
    print(f"Count: {metrics['Count']}")
    print(f"Frequency: {metrics['Frequency']}")
    print(f"Proportion: {metrics['Proportion']}")
    print(f"Minimum: {metrics['Minimum']}")
    print(f"Maximum: {metrics['Maximum']}")
    print(f"Mode: {metrics['Mode']}")
    print(f"Rank: {metrics['Rank']}")
    print(f"Spearman Correlation: {metrics.get('Spearman Correlation', 'N/A')}")
    print(f"Kendall Correlation: {metrics.get('Kendall Correlation', 'N/A')}")

"""### **Non-Categorical Variable**
#### 1. Measures of Central Tendency {Minimum | Maximum | Mean | Median | Mode | Percentile}
#### 2. Measures of Dispersion {Range | Standard Deviation | Skewness | Kurtosis | Correlation (Pearson | Spearman)}
#### 3. Composite Measures {Coefficient of Variation | Confidence Interval}

"""

def non_categorical_analysis(data, numerical_cols):
    """Analyzes non-categorical (numerical) variables for various metrics.

    Args:
        data: pandas DataFrame containing the dataset.
        numerical_cols: List of numerical columns to analyze.

    Returns:
        Dictionary containing metrics for each numerical column.
    """
    analysis_results = {}

    # Ensure that only numerical columns are being processed
    numerical_cols = [col for col in numerical_cols if pd.api.types.is_numeric_dtype(data[col])]  # Filter numerical columns

    for col in numerical_cols:  # Iterate only over numerical columns
        # Check if the column contains non-numeric data
        if not pd.api.types.is_numeric_dtype(data[col]):
            print(f"Warning: Column '{col}' contains non-numeric data. It will be excluded from analysis.")
            continue  # Skip non-numeric columns

        # Convert non-numeric values to NaN (if any)
        data[col] = pd.to_numeric(data[col], errors='coerce')

        # Drop NaN values for the analysis
        col_data = data[col].dropna()

        result = {}

        # Measures of Central Tendency
        result['Minimum'] = col_data.min()
        result['Maximum'] = col_data.max()
        result['Mean'] = col_data.mean()
        result['Median'] = col_data.median()
        result['Mode'] = col_data.mode().iloc[0] if not col_data.mode().empty else None
        result['Percentile (25th)'] = np.percentile(col_data, 25)
        result['Percentile (50th)'] = np.percentile(col_data, 50)
        result['Percentile (75th)'] = np.percentile(col_data, 75)

        # Measures of Dispersion
        result['Range'] = col_data.max() - col_data.min()
        result['Standard Deviation'] = col_data.std()
        result['Skewness'] = col_data.skew()
        result['Kurtosis'] = col_data.kurtosis()

        # Correlation (Pearson and Spearman) - Calculate only for numerical columns
        correlation_pearson = data[rb36tn52_numerical_cols].corr(method='pearson')  # Use filtered numerical columns
        correlation_spearman = data[rb36tn52_numerical_cols].corr(method='spearman')  # Use filtered numerical columns
        result['Correlation (Pearson)'] = correlation_pearson[col].to_dict()
        result['Correlation (Spearman)'] = correlation_spearman[col].to_dict()

        # Composite Measures
        result['Coefficient of Variation'] = result['Standard Deviation'] / result['Mean'] if result['Mean'] != 0 else np.nan

        # Confidence Interval (95%)
        confidence_interval = stats.t.interval(0.95, len(col_data) - 1, loc=result['Mean'], scale=stats.sem(col_data))
        result['Confidence Interval (95%)'] = confidence_interval

        analysis_results[col] = result

    return analysis_results


# To display the metrics for each numerical variable in your dataset
rb36tn52_numerical_cols = ['Quantity', 'Value', 'Weight', 'Invoice_Number', 'Customs_Code']  # Update with your actual numerical columns
rb36tn52_numerical_metrics = non_categorical_analysis(rb36tn52_dfs, rb36tn52_numerical_cols)

# Displaying the results for each numerical variable
for col, metrics in rb36tn52_numerical_metrics.items():
    print(f"\nMetrics for {col}:")
    print(f"Minimum: {metrics['Minimum']}")
    print(f"Maximum: {metrics['Maximum']}")
    print(f"Mean: {metrics['Mean']}")
    print(f"Median: {metrics['Median']}")
    print(f"Mode: {metrics['Mode']}")
    print(f"Percentile (25th): {metrics['Percentile (25th)']}")
    print(f"Percentile (50th): {metrics['Percentile (50th)']}")
    print(f"Percentile (75th): {metrics['Percentile (75th)']}")
    print(f"Range: {metrics['Range']}")
    print(f"Standard Deviation: {metrics['Standard Deviation']}")
    print(f"Skewness: {metrics['Skewness']}")
    print(f"Kurtosis: {metrics['Kurtosis']}")
    print(f"Correlation (Pearson): {metrics['Correlation (Pearson)']}")
    print(f"Correlation (Spearman): {metrics['Correlation (Spearman)']}")
    print(f"Coefficient of Variation: {metrics['Coefficient of Variation']}")
    print(f"Confidence Interval (95%): {metrics['Confidence Interval (95%)']}")

"""### **Data Visualization**

#### **Basic Plots**
#### Bar | Pie | Scatter | Line
"""

# Basic Plots
import plotly.express as px
# 1. Bar Plot for 'Payment_Terms' (Matplotlib)
plt.figure(figsize=(10, 6))
sns.countplot(x='Payment_Terms', data=rb36tn52_dfs, palette='Set2')  # Vibrant colors
plt.title('Bar Plot: Payment Terms Distribution')
plt.xticks(rotation=45)
plt.show()

# 2. Pie Plot for 'Category' (Plotly)
rb36tn52_category_counts = rb36tn52_dfs['Category'].value_counts()
fig = px.pie(rb36tn52_category_counts, names=rb36tn52_category_counts.index, values=rb36tn52_category_counts.values,
             title='Pie Plot: Category Distribution', color_discrete_sequence=px.colors.qualitative.Set3)
fig.update_traces(textinfo='percent+label')
fig.show()

# 3. Scatter Plot for 'Quantity' vs 'Value' (Plotly)
fig = px.scatter(rb36tn52_dfs, x='Quantity', y='Value', title='Scatter Plot: Quantity vs. Value',
                 color='Value', color_continuous_scale='Viridis')
fig.show()

# 4. Line Plot for 'Import_Export' over Time (Plotly)

# Convert 'Date' to datetime
rb36tn52_dfs['Date'] = pd.to_datetime(rb36tn52_dfs['Date'], errors='coerce')

# Extract the year from 'Date'
rb36tn52_dfs['Year'] = rb36tn52_dfs['Date'].dt.year

# Group by year and Import_Export to calculate the total values (e.g., sum of Quantity or another metric)
rb36tn52_aggregated_data = rb36tn52_dfs.groupby(['Year', 'Import_Export'], as_index=False).size()

# Rename the aggregation column
rb36tn52_aggregated_data.rename(columns={'size': 'Count'}, inplace=True)

# Create a line plot
fig = px.line(rb36tn52_aggregated_data,
              x='Year',
              y='Count',
              color='Import_Export',
              title='Import and Export Trends Over Years',
              labels={'Count': 'Total Transactions'},
              line_shape='linear')

fig.show()

"""#### **Advance Plots**
#### Box-Whisker | Pair | Heat

"""

import warnings
# Suppress specific warning
warnings.filterwarnings("ignore", message="Ignoring `palette` because no `hue` variable has been assigned.")

# 1. Box-Whisker Plot for 'Shipping_Method' (Matplotlib)
plt.figure(figsize=(10, 6))
sns.boxplot(x='Shipping_Method', y='Value', data=rb36tn52_dfs, palette='Set1')  # Vibrant colors
plt.title('Box-Whisker Plot: Value by Shipping Method')
plt.xticks(rotation=45)
plt.show()

# 2. Pair Plot for Numerical Data (Seaborn)
rb36tn52_numerical_cols = ['Quantity', 'Value', 'Weight', 'Invoice_Number', 'Customs_Code']
sns.pairplot(rb36tn52_dfs[rb36tn52_numerical_cols], palette='coolwarm')  # Vibrant colors
plt.title('Pair Plot: Numerical Variables')
plt.show()

# 3. Heatmap for Correlation between Numerical Variables (Seaborn)
plt.figure(figsize=(10, 6))
rb36tn52_corr_matrix = rb36tn52_dfs[rb36tn52_numerical_cols].corr()  # Correlation matrix
sns.heatmap(rb36tn52_corr_matrix, annot=True, cmap='YlGnBu', fmt='.2f', linewidths=0.5)  # Vibrant colors
plt.title('Heatmap: Correlation Matrix of Numerical Variables')
plt.show()

"""### **Inferential Statistics**

#### **Categorical Variable [Nominal | Ordinal]**
#### ●	Test of Homogeneity {Chi-sq}
"""

# Create a contingency table
rb36tn52_contingency_table = pd.crosstab(rb36tn52_dfs['Category'], rb36tn52_dfs['Payment_Terms'])

# Perform Chi-Square Test of Homogeneity
chi2_stat, p_val, dof, expected = stats.chi2_contingency(rb36tn52_contingency_table)

# Display the results
print(f"Chi-Square Statistic: {chi2_stat}")
print(f"P-Value: {p_val}")
print(f"Degrees of Freedom: {dof}")
print(f"Expected Frequencies: \n{expected}")

# Interpret the result
if p_val < 0.05:
    print("Reject the null hypothesis: There is a significant difference in the distribution of 'Payment_Terms' across 'Category'.")
else:
    print("Fail to reject the null hypothesis: There is no significant difference in the distribution of 'Payment_Terms' across 'Category'.")

"""#### **Non-Categorical Variable**
#### ●	Test of Normality {Shapiro-Wilk | Kolmogorov-Smirnov | Anderson-Darling | Jarque-Bera}
"""

# 1. Shapiro-Wilk Test
shapiro_stat, shapiro_p = stats.shapiro(rb36tn52_dfs['Value'])
print("\n--- Shapiro-Wilk Test ---")
print(f"Statistic: {shapiro_stat:.4f}, p-value: {shapiro_p:.4f}")
if shapiro_p < 0.05:
    print("Interpretation: Reject null hypothesis, the data is not normally distributed.")
else:
    print("Interpretation: Fail to reject null hypothesis, the data is normally distributed.")

# 2. Kolmogorov-Smirnov Test
ks_stat, ks_p = stats.kstest(rb36tn52_dfs['Value'], 'norm')
print("\n--- Kolmogorov-Smirnov Test ---")
print(f"Statistic: {ks_stat:.4f}, p-value: {ks_p:.4f}")
if ks_p < 0.05:
    print("Interpretation: Reject null hypothesis, the data is not normally distributed.")
else:
    print("Interpretation: Fail to reject null hypothesis, the data is normally distributed.")

# 3. Anderson-Darling Test
anderson_result = stats.anderson(rb36tn52_dfs['Value'], dist='norm')
print("\n--- Anderson-Darling Test ---")
print(f"Statistic: {anderson_result.statistic:.4f}")
print(f"Critical Values: {anderson_result.critical_values}")
print(f"Significance Levels: {anderson_result.significance_level}")
# Interpretation based on significance levels
if anderson_result.statistic > anderson_result.critical_values[2]:  # 5% significance level
    print("Interpretation: Reject null hypothesis, the data is not normally distributed.")
else:
    print("Interpretation: Fail to reject null hypothesis, the data is normally distributed.")

# 4. Jarque-Bera Test
jb_stat, jb_p = stats.jarque_bera(rb36tn52_dfs['Value'])
print("\n--- Jarque-Bera Test ---")
print(f"Statistic: {jb_stat:.4f}, p-value: {jb_p:.4f}")
if jb_p < 0.05:
    print("Interpretation: Reject null hypothesis, the data is not normally distributed.")
else:
    print("Interpretation: Fail to reject null hypothesis, the data is normally distributed.")

"""#### **Non-Categorical Variable**
#### ●	Test of Correlation {t}
"""

# Pearson Correlation Test for 'Quantity' and 'Value'
pearson_corr, _ = stats.pearsonr(rb36tn52_dfs['Quantity'], rb36tn52_dfs['Value'])
print(f"Pearson Correlation: {pearson_corr}")

# Interpretation:
if abs(pearson_corr) > 0.8:
    print("Strong correlation between 'Quantity' and 'Value'.")
elif abs(pearson_corr) > 0.5:
    print("Moderate correlation between 'Quantity' and 'Value'.")
else:
    print("Weak correlation between 'Quantity' and 'Value'.")

from scipy import stats

# t-Test for independent samples between 'Quantity' and 'Value'
t_stat, p_val = stats.ttest_ind(rb36tn52_dfs['Quantity'], rb36tn52_dfs['Value'])
print(f"t-Test: t-statistic = {t_stat}, p-value = {p_val}")

# Interpretation:
if p_val < 0.05:
    print("Reject the null hypothesis: There is a significant difference in means between 'Quantity' and 'Value'.")
else:
    print("Fail to reject the null hypothesis: There is no significant difference in means between 'Quantity' and 'Value'.")

"""### **Machine Learning Models**

#### **Unsupervised Machine Learning**

#### **Dimensity Reduction: Records**
#### ●	Clustering {Hierarchical (H) | K-Means (KM) | (Others: DBSCAN | BIRCH)}

**Hierarchical (H)**
"""

from sklearn.cluster import AgglomerativeClustering
import matplotlib.pyplot as plt
import seaborn as sns

# Perform Hierarchical Clustering
rb36tn52_hierarchical = AgglomerativeClustering(n_clusters=3)  # Change number of clusters as needed
rb36tn52_dfs['Hierarchical_Cluster'] = rb36tn52_hierarchical.fit_predict(rb36tn52_dfs[['Quantity', 'Value', 'Weight']])  # Add more features if necessary

# Check if the column 'Hierarchical_Cluster' is added
print("Columns in the DataFrame after Hierarchical Clustering:", rb36tn52_dfs.columns)  # Verify the column is present

# Visualize the clusters
plt.figure(figsize=(8, 6))
sns.scatterplot(x='Quantity', y='Value', hue='Hierarchical_Cluster', data=rb36tn52_dfs, palette='Set1')
plt.title('Hierarchical Clustering')
plt.xlabel('Quantity')
plt.ylabel('Value')
plt.legend(title='Hierarchical Cluster')
plt.show()

# Interpretation of Hierarchical Clustering
print("\nInterpretation:")
print("1. Hierarchical Clustering organizes data into a tree-like structure, known as a dendrogram.")
print("2. Agglomerative Clustering starts with each data point as its own cluster and merges them iteratively based on similarity.")
print("3. In this implementation, we specified 3 clusters (`n_clusters=3`).")
print("4. The scatter plot shows the relationship between 'Quantity' and 'Value', with points colored by their hierarchical cluster assignments.")
print("5. This method is useful for identifying nested groupings in data and does not require specifying the number of clusters initially.")
print("6. Hierarchical Clustering is sensitive to the distance metric used; here, the default is Euclidean distance.")

"""**K-Means (KM)**"""

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns

# Perform K-Means Clustering
rb36tn52_kmeans = KMeans(n_clusters=3, random_state=36)
rb36tn52_dfs['KMeans_Cluster'] = rb36tn52_kmeans.fit_predict(rb36tn52_dfs[['Quantity', 'Value', 'Weight']])  # Ensure the correct features are used

# Check if the column 'KMeans_Cluster' is added
print("Columns in the DataFrame after K-Means Clustering:", rb36tn52_dfs.columns)  # Verify the column is present

# Visualize the clusters
plt.figure(figsize=(8, 6))
sns.scatterplot(x='Quantity', y='Value', hue='KMeans_Cluster', data=rb36tn52_dfs, palette='Set2')
plt.title('K-Means Clustering')
plt.xlabel('Quantity')
plt.ylabel('Value')
plt.legend(title='K-Means Cluster')
plt.show()

# Interpretation of K-Means Clustering
print("\nInterpretation:")
print("1. K-Means Clustering partitions the data into 3 clusters (as specified by n_clusters).")
print("2. Each cluster is represented by a centroid, and data points are assigned to the nearest centroid.")
print("3. The scatter plot shows the relationship between 'Quantity' and 'Value', with each point colored according to its cluster assignment.")
print("4. The clustering algorithm assumes clusters are spherical and equally sized.")
print("5. K-Means is sensitive to outliers; outliers may significantly influence the cluster centroids.")
print("6. Clustering results can be used for segmentation, pattern identification, or further analysis.")

"""**DBSCAN**"""

from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plt
import seaborn as sns

# Perform DBSCAN Clustering
rb36tn52_dbscan = DBSCAN(eps=0.5, min_samples=5)  # You can adjust eps and min_samples
rb36tn52_dfs['DBSCAN_Cluster'] = rb36tn52_dbscan.fit_predict(rb36tn52_dfs[['Quantity', 'Value', 'Weight']])  # Add more features if necessary

# Visualize the clusters
plt.figure(figsize=(8, 6))
sns.scatterplot(x='Quantity', y='Value', hue='DBSCAN_Cluster', data=rb36tn52_dfs, palette='Set3')
plt.title('DBSCAN Clustering')
plt.xlabel('Quantity')
plt.ylabel('Value')
plt.legend(title='DBSCAN Cluster')
plt.show()

# Interpretation of DBSCAN Clustering
print("\nInterpretation:")
print("1. DBSCAN (Density-Based Spatial Clustering of Applications with Noise) identifies clusters of high density and separates noise points.")
print("2. The scatter plot shows the relationship between 'Quantity' and 'Value', with each point colored according to its assigned cluster.")
print("3. Points labeled as '-1' are considered noise and do not belong to any cluster.")
print("4. Clusters represent groups of data points that are close to each other based on the 'eps' distance parameter.")
print("5. The parameters 'eps' (radius of neighborhood) and 'min_samples' (minimum points to form a cluster) can be adjusted to refine the clustering.")
print("6. DBSCAN is particularly useful for identifying clusters in data with varying densities and handling outliers effectively.")

"""**BIRCH**"""

from sklearn.cluster import Birch
import matplotlib.pyplot as plt
import seaborn as sns

# Perform BIRCH Clustering
rb36tn52_birch = Birch(n_clusters=3)  # Adjust n_clusters based on your data
rb36tn52_dfs['BIRCH_Cluster'] = rb36tn52_birch.fit_predict(rb36tn52_dfs[['Quantity', 'Value', 'Weight']])

# Visualize the clusters
plt.figure(figsize=(8, 6))
sns.scatterplot(x='Quantity', y='Value', hue='BIRCH_Cluster', data=rb36tn52_dfs, palette='Paired')
plt.title('BIRCH Clustering')
plt.xlabel('Quantity')
plt.ylabel('Value')
plt.legend(title='BIRCH Cluster')
plt.show()

# Interpretation of BIRCH Clustering
print("\nInterpretation:")
print("1. BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies) groups the data into clusters based on hierarchical clustering principles.")
print("2. The scatter plot displays the relationship between 'Quantity' and 'Value', with each point colored according to its assigned cluster.")
print("3. Each cluster represents a group of data points with similar characteristics in terms of 'Quantity' and 'Value'.")
print("4. Clusters can help identify patterns or trends in the data, such as high-value and low-quantity transactions or vice versa.")
print("5. The number of clusters (3 in this case) can be adjusted to better fit the data distribution.")

"""**PCA & K-MEANS Clustering**"""

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns

# Perform PCA for dimensionality reduction
rb36tn52_pca = PCA(n_components=2)  # Reduce to 2 dimensions for visualization
rb36tn52_pca_components = rb36tn52_pca.fit_transform(rb36tn52_dfs[['Quantity', 'Value', 'Weight']])

# Add PCA components to the DataFrame
rb36tn52_dfs['PCA_1'] = rb36tn52_pca_components[:, 0]
rb36tn52_dfs['PCA_2'] = rb36tn52_pca_components[:, 1]

# Visualize the reduced data (PCA)
plt.figure(figsize=(8, 6))
sns.scatterplot(x='PCA_1', y='PCA_2', hue='KMeans_Cluster', data=rb36tn52_dfs, palette='viridis')
plt.title('PCA and K-Means Clustering')
plt.show()

# Interpretation of PCA and K-Means Clustering
print("\nInterpretation:")
print("1. PCA reduces the data to two dimensions (PC1 and PC2) for easier visualization.")
print("2. The scatter plot shows the distribution of data points in this 2D space.")
print("3. Points that are close to each other in the plot are similar, while points that are farther apart are more distinct.")
print("4. K-Means clustering has grouped the data into 3 clusters based on the reduced dimensions (PCA_1 and PCA_2).")
print("5. The different colors represent different clusters, which could indicate distinct patterns or behaviors in the data.")
print("6. You can use these clusters to analyze the data further, identify patterns, or make decisions based on cluster membership.")

"""#### **Dimensity Reduction: Variables**

●	Principal Component Analysis
"""

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

# Suppress specific FutureWarning
warnings.filterwarnings("ignore", category=FutureWarning)

# Select numerical columns for PCA
rb36tn52_numerical_cols = ['Quantity', 'Value', 'Weight']

# Standardize the numerical data
rb36tn52_scaler = StandardScaler()
rb36tn52_scaled_data = rb36tn52_scaler.fit_transform(rb36tn52_dfs[rb36tn52_numerical_cols])

# Apply PCA
rb36tn52_pca = PCA(n_components=2)  # Reduce to 2 principal components for visualization
rb36tn52_pca_components = rb36tn52_pca.fit_transform(rb36tn52_scaled_data)

# Create a DataFrame with the principal components
rb36tn52_pca_df = pd.DataFrame(data=rb36tn52_pca_components, columns=['PC1', 'PC2'])

# Visualize the explained variance ratio
plt.figure(figsize=(8, 6))
sns.barplot(x=['PC1', 'PC2'], y=rb36tn52_pca.explained_variance_ratio_, palette='viridis')
plt.title('Explained Variance Ratio of Principal Components')
plt.ylabel('Variance Explained')
plt.show()

# Visualize the data in the 2D PCA space
plt.figure(figsize=(8, 6))
sns.scatterplot(x='PC1', y='PC2', data=rb36tn52_pca_df, palette='viridis')
plt.title('PCA: Data in 2D Space')
plt.show()

# Interpretation of Explained Variance Ratio
print(f"Explained Variance Ratio:\nPC1: {rb36tn52_pca.explained_variance_ratio_[0]:.4f}, PC2: {rb36tn52_pca.explained_variance_ratio_[1]:.4f}")
print("\nInterpretation:")
if rb36tn52_pca.explained_variance_ratio_[0] > 0.7:
    print("PC1 explains a significant portion of the variance (over 70%). It is the most important component.")
else:
    print("PC1 explains a moderate portion of the variance. Both components are important.")

"""### **Model Performance Metrics**

#### **Unsupervised Machine Learning**
#### ●	Clustering: Silhouette Coefficient | Davies-Bouldin Index
"""

from sklearn.metrics import silhouette_score, davies_bouldin_score
from sklearn.cluster import KMeans

rb36tn52_clustering_features = ['Quantity', 'Value', 'Weight']

# Perform K-Means Clustering as an example
rb36tn52_kmeans = KMeans(n_clusters=3, random_state=42)
rb36tn52_dfs['KMeans_Cluster'] = rb36tn52_kmeans.fit_predict(rb36tn52_dfs[rb36tn52_clustering_features])

# Silhouette Coefficient
rb36tn52_silhouette_avg = silhouette_score(rb36tn52_dfs[rb36tn52_clustering_features], rb36tn52_dfs['KMeans_Cluster'])
print(f"Silhouette Coefficient: {rb36tn52_silhouette_avg:.2f}")

# Davies-Bouldin Index
rb36tn52_davies_bouldin = davies_bouldin_score(rb36tn52_dfs[rb36tn52_clustering_features], rb36tn52_dfs['KMeans_Cluster'])
print(f"Davies-Bouldin Index: {rb36tn52_davies_bouldin:.2f}")

# Interpretation
print("\nInterpretation:")
print("1. Silhouette Coefficient:")
print("   - Measures how similar an object is to its own cluster compared to other clusters.")
print("   - Values range from -1 to 1. Higher values indicate better-defined clusters.")
print(f"   - In this case, the Silhouette Coefficient is {rb36tn52_silhouette_avg:.2f}, indicating {'well-defined clusters' if rb36tn52_silhouette_avg > 0.5 else 'poorly defined clusters'}.")

print("\n2. Davies-Bouldin Index:")
print("   - Measures the average similarity ratio of each cluster with the most similar cluster.")
print("   - Lower values indicate better clustering (lower intra-cluster distance and higher inter-cluster distance).")
print(f"   - In this case, the Davies-Bouldin Index is {rb36tn52_davies_bouldin:.2f}, indicating {'good clustering' if rb36tn52_davies_bouldin < 1 else 'room for improvement in clustering'}.")

"""#### **Model Run Statistics**
●	Time Taken (Lesser the Better)
●	Memory Used (Lesser the Better)
●	Inherent Complexity

"""

import time
import psutil
import os
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans

# Function to track model statistics
def model_run_statistics(rb36tn52_model, rb36tn52_data, rb36tn52_model_name="Model"):
    rb36tn52_start_time = time.time()  # Start time tracking
    rb36tn52_process = psutil.Process(os.getpid())  # Get the current process

    # Run the model
    rb36tn52_model.fit(rb36tn52_data)

    rb36tn52_end_time = time.time()  # End time tracking
    rb36tn52_time_taken = rb36tn52_end_time - rb36tn52_start_time  # Time taken in seconds

    # Memory used (in MB)
    rb36tn52_memory_used = rb36tn52_process.memory_info().rss / (1024 * 1024)  # Memory used in MB

    # Inherent complexity based on model type
    if isinstance(rb36tn52_model, KMeans):
        rb36tn52_inherent_complexity = "Moderate (KMeans)"
    elif isinstance(rb36tn52_model, PCA):
        rb36tn52_inherent_complexity = "Low (PCA)"
    else:
        rb36tn52_inherent_complexity = "Unknown"

    # Print the statistics
    print(f"\n{rb36tn52_model_name} Run Statistics:")
    print(f"Time Taken: {rb36tn52_time_taken:.4f} seconds")
    print(f"Memory Used: {rb36tn52_memory_used:.2f} MB")
    print(f"Inherent Complexity: {rb36tn52_inherent_complexity}")

# Example of running KMeans and PCA and tracking the stats
rb36tn52_data = rb36tn52_dfs[['Quantity', 'Value', 'Weight']]

# KMeans Example
rb36tn52_kmeans = KMeans(n_clusters=3, random_state=42)
model_run_statistics(rb36tn52_kmeans, rb36tn52_data, rb36tn52_model_name="KMeans Clustering")

# PCA Example
rb36tn52_pca = PCA(n_components=2)
model_run_statistics(rb36tn52_pca, rb36tn52_data, rb36tn52_model_name="PCA")

"""### **Diagnostics**
#### **Unsupervised Machine Learning**
●	Clustering: Cluster Characteristics {Nature of each Cluster (Homogeneity) | Difference between Clusters (Heterogeneity)}

"""

from sklearn.metrics import silhouette_score

data = rb36tn52_dfs[['Quantity', 'Value', 'Weight']]

# Perform KMeans Clustering
rb36tn52_kmeans = KMeans(n_clusters=3, random_state=42)
rb36tn52_dfs['KMeans_Cluster'] = rb36tn52_kmeans.fit_predict(data)

# Perform BIRCH Clustering
rb36tn52_birch = Birch(n_clusters=3)
rb36tn52_dfs['BIRCH_Cluster'] = rb36tn52_birch.fit_predict(data)

# Perform DBSCAN Clustering
rb36tn52_dbscan = DBSCAN(eps=0.5, min_samples=5)
rb36tn52_dfs['DBSCAN_Cluster'] = rb36tn52_dbscan.fit_predict(data)

# Perform Agglomerative Clustering
rb36tn52_hierarchical = AgglomerativeClustering(n_clusters=3)
rb36tn52_dfs['Hierarchical_Cluster'] = rb36tn52_hierarchical.fit_predict(data)

# Function to calculate Homogeneity (Silhouette Score)
def rb36tn52_homogeneity_score(data, cluster_labels):
    # For DBSCAN, exclude noise points labeled as -1
    if len(np.unique(cluster_labels)) > 1:
        return silhouette_score(data, cluster_labels)
    else:
        return None  # Return None if there's only one cluster

# Function to calculate Heterogeneity (Distance between cluster centers)
def rb36tn52_heterogeneity_score(model, data, cluster_labels):
    centers = model.cluster_centers_ if hasattr(model, 'cluster_centers_') else np.array([data[cluster_labels == i].mean(axis=0) for i in np.unique(cluster_labels)])
    distances = np.linalg.norm(centers[:, None] - centers, axis=2)  # Pairwise distance between cluster centers
    return distances

# Analyze KMeans Clustering
print("KMeans Clustering Analysis:")
rb36tn52_kmeans_homogeneity = rb36tn52_homogeneity_score(data, rb36tn52_dfs['KMeans_Cluster'])
if rb36tn52_kmeans_homogeneity is not None:
    print(f"Homogeneity (Silhouette Score): {rb36tn52_kmeans_homogeneity:.4f}")
    if rb36tn52_kmeans_homogeneity > 0.5:
        print("Interpretation: The KMeans clusters are well-separated and homogeneous, indicating distinct groupings.")
    else:
        print("Interpretation: The KMeans clusters are not very well-separated, suggesting overlap between clusters.")
else:
    print("Homogeneity (Silhouette Score): Not applicable (only one cluster).")
    print("Interpretation: KMeans resulted in a single cluster, suggesting that the data may not have distinct groupings.")

rb36tn52_kmeans_heterogeneity = rb36tn52_heterogeneity_score(rb36tn52_kmeans, data, rb36tn52_dfs['KMeans_Cluster'])
print(f"Heterogeneity (Cluster Center Distance):\n{rb36tn52_kmeans_heterogeneity}")
print("Interpretation: The distance between cluster centers indicates the separation between clusters. Larger distances suggest more distinct clusters.")

# Analyze BIRCH Clustering
print("BIRCH Clustering Analysis:")
rb36tn52_birch_homogeneity = rb36tn52_homogeneity_score(data, rb36tn52_dfs['BIRCH_Cluster'])
if rb36tn52_birch_homogeneity is not None:
    print(f"Homogeneity (Silhouette Score): {rb36tn52_birch_homogeneity:.4f}")
    if rb36tn52_birch_homogeneity > 0.5:
        print("Interpretation: The BIRCH clusters are well-separated and homogeneous, suggesting distinct groupings.")
    else:
        print("Interpretation: The BIRCH clusters show some overlap, indicating that the data may not be well-separated.")
else:
    print("Homogeneity (Silhouette Score): Not applicable (only one cluster).")
    print("Interpretation: BIRCH resulted in a single cluster, indicating the lack of distinct groupings in the data.")

rb36tn52_birch_heterogeneity = rb36tn52_heterogeneity_score(rb36tn52_birch, data, rb36tn52_dfs['BIRCH_Cluster'])
print(f"Heterogeneity (Cluster Center Distance):\n{rb36tn52_birch_heterogeneity}")
print("Interpretation: The distance between cluster centers suggests the degree of separation. A higher distance indicates more distinct clusters.")

# Analyze DBSCAN Clustering (DBSCAN doesn't have cluster centers, so use mean of points in each cluster)
print("DBSCAN Clustering Analysis:")
rb36tn52_dbscan_homogeneity = rb36tn52_homogeneity_score(data, rb36tn52_dfs['DBSCAN_Cluster'])
if rb36tn52_dbscan_homogeneity is not None:
    print(f"Homogeneity (Silhouette Score): {rb36tn52_dbscan_homogeneity:.4f}")
    if rb36tn52_dbscan_homogeneity > 0.5:
        print("Interpretation: DBSCAN clusters are well-separated and homogeneous, indicating distinct groupings.")
    else:
        print("Interpretation: DBSCAN clusters are not well-separated, suggesting overlap between clusters.")
else:
    print("Homogeneity (Silhouette Score): Not applicable (only one cluster).")
    print("Interpretation: DBSCAN resulted in a single cluster or noise, suggesting that the data may not have distinct groupings or contains outliers.")

# DBSCAN doesn't have cluster centers, calculate heterogeneity manually
rb36tn52_dbscan_heterogeneity = np.array([data[rb36tn52_dfs['DBSCAN_Cluster'] == i].mean(axis=0) for i in np.unique(rb36tn52_dfs['DBSCAN_Cluster']) if i != -1])
print(f"Heterogeneity (Cluster Center Distance):\n{rb36tn52_dbscan_heterogeneity}")
print("Interpretation: The heterogeneity for DBSCAN is calculated based on the mean of points in each cluster. Larger distances between cluster centers indicate better separation.")

# Analyze Hierarchical Clustering
print("Hierarchical Clustering Analysis:")
rb36tn52_hierarchical_homogeneity = rb36tn52_homogeneity_score(data, rb36tn52_dfs['Hierarchical_Cluster'])
if rb36tn52_hierarchical_homogeneity is not None:
    print(f"Homogeneity (Silhouette Score): {rb36tn52_hierarchical_homogeneity:.4f}")
    if rb36tn52_hierarchical_homogeneity > 0.5:
        print("Interpretation: The Hierarchical clusters are well-separated and homogeneous, indicating distinct groupings.")
    else:
        print("Interpretation: The Hierarchical clusters show overlap, suggesting that the data may not be well-separated.")
else:
    print("Homogeneity (Silhouette Score): Not applicable (only one cluster).")
    print("Interpretation: Hierarchical clustering resulted in a single cluster, suggesting that the data may not have distinct groupings.")

rb36tn52_hierarchical_heterogeneity = rb36tn52_heterogeneity_score(rb36tn52_hierarchical, data, rb36tn52_dfs['Hierarchical_Cluster'])
print(f"Heterogeneity (Cluster Center Distance):\n{rb36tn52_hierarchical_heterogeneity}")
print("Interpretation: The heterogeneity for Hierarchical clustering is calculated based on the distance between cluster centers. A larger distance indicates better separation between clusters.")

